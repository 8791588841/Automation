{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94a2b77",
   "metadata": {},
   "source": [
    "This project focus on building a machine learning workflow that will run autonomously with the CSV file and return the best-performing model.\n",
    "\n",
    "##### SkillsUsed : XGBoost, LogisticRegression, KNN, SVM, EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3cc8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9718236",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the datasets.\n",
    "\n",
    "df1 = pd.read_csv('C:\\Automation\\TelcomCustomer-Churn_1.csv')\n",
    "df2 = pd.read_csv('C:\\Automation\\TelcomCustomer-Churn_2.csv')\n",
    "df3 = pd.merge(df1, df2, on=\"customerID\")\n",
    "\n",
    "#Dropping the last column considering it to be the target variable column in csv file.\n",
    "\n",
    "df_original = df3.drop(df3.iloc[:,-1:],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee95a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are defining a class and building main function in it.\n",
    "#We will keep on building the required functions in main function.\n",
    "#We will build the models as independent functions in the class which will not belong to main.\n",
    "\n",
    "class Workflow:\n",
    "  \n",
    "  #Defining a initialization function for the dataframe.\n",
    "  #df_all contains all the columns.\n",
    "  #df contains all the columns except the target variable.\n",
    "\n",
    "  def initialize(self,df3,df_original):\n",
    "    global df_all\n",
    "    df_all = df3\n",
    "    global df\n",
    "    df = df_original\n",
    "\n",
    "  def main(self):\n",
    "\n",
    "    #We are creating a function wherein we are removing the columns like ID or Index which will not have any impact in evaluation of models.\n",
    "    #We have iterated through the columns and if the column endswith the following strings, we will remove those columns.\n",
    "\n",
    "    def remove_cols(df):\n",
    "      k = [x for x in df.columns if x.endswith(\"ID\") | x.endswith(\"id\") | x.endswith(\"Id\") | x.endswith(\"iD\") | x.endswith(\"Index\") \n",
    "          | x.endswith(\"INDEX\") | x.endswith(\"index\") ] \n",
    "      for k in k:\n",
    "        df.drop(k,inplace=True,axis=1)\n",
    "\n",
    "    #We have defined a function data_types which will traverse through the values of each column in dataframe and if it finds \n",
    "    #values that can be converted to float(i.e. numbers or floats), it will convert them to float and the datatype will be updated to float.\n",
    "    #This is very useful when we have raw data with columns having dtype as object or something else which is not a float/int, \n",
    "    #we can traverse through the columns and assign proper datatypes to the column with the correct updated values.\n",
    "    #We have taken a measure if the column contains more than 80% of float/int values, then only we will convert it to float.\n",
    "    #The reason is that it will ensure that the column has majority of float values and we are considering the column as float column.\n",
    "    #We have defined conv_float function inside the data_types function which will return true if it sucessfully convert the value to float\n",
    "    #or else false.\n",
    "\n",
    "    def data_types(df):\n",
    "      def conv_float(s):\n",
    "        try:\n",
    "          float(s)\n",
    "          return True\n",
    "        except ValueError:\n",
    "          return False\n",
    "      y=[]\n",
    "      try:\n",
    "        for x in df.columns:\n",
    "          for z in df[x]:\n",
    "            y.append(conv_float(z))\n",
    "          if y.count(True)>(.08*len(df[x])):\n",
    "            df[x]=df[x].apply(pd.to_numeric, errors='coerce')\n",
    "          else:\n",
    "            continue\n",
    "          y=[]\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "    #We are defining a function remove_null which replaces the null values with mean for numerical values and median for categorical values.\n",
    "    #We have taken the sum of null values in all the columns and placed them in a dictionary. We have iterated the dictionary and if there are columns \n",
    "    #whose sum is greater than 0, i.e., the column has null values, we have kept them in keys.\n",
    "    #We have further classified the keys(columns) to numerical and categorical keys so that we can apply mean and median on the right columns.\n",
    "\n",
    "    def remove_null(df):\n",
    "      dicts = {}\n",
    "      dicts = df.isnull().sum()\n",
    "      keys = [key for key, value in dicts.items() if value>0]\n",
    "      if len(keys)==0:\n",
    "        return\n",
    "      else:\n",
    "        num_keys = [key for key in keys if (df[key].dtype=='int64') | (df[key].dtype==float)]\n",
    "        cat_keys = [key for key in keys if (df[key].dtype==object)]\n",
    "      if len(num_keys)!=0:\n",
    "        for k in num_keys:\n",
    "          df[k].fillna(df[k].mean(),inplace=True)\n",
    "      if len(cat_keys)!=0:\n",
    "        for k in cat_keys:\n",
    "          df[k].fillna(df[k].median(),inplace=True)\n",
    "\n",
    "    \n",
    "\n",
    "    #We are defining a function standard for standarization which will transform the data with mean of 0 and standard deviation of 1.\n",
    "    #We will use Z-Score method.\n",
    "    #Extracting columns with int and float datatypes.\n",
    "    #Standardizing the columns by subtracting the values with mean and then dividing by standard deviation.\n",
    "\n",
    "    def standard(df):\n",
    "      keys = [key for key in df.columns if (df[key].dtype==float) | (df[key].dtype=='int64')]\n",
    "      for k in keys:\n",
    "        df[k] = (df[k]-df[k].mean())/df[k].std()\n",
    "\n",
    "    #We are defining a function called preprocessing which will call all the functions defined under main function.\n",
    "  \n",
    "    def preprocessing(df):\n",
    "      remove_cols(df)\n",
    "      data_types(df)\n",
    "      remove_null(df)\n",
    "      standard(df)\n",
    "      \n",
    "    #Calling the preprocessing function to make the dataset clean and ready for the models.\n",
    "\n",
    "    preprocessing(df)\n",
    "\n",
    "  #Now we will define different models.\n",
    "  \n",
    "  def xg_boost(self,df):\n",
    "    #Importing the xgboost and XGBClassifier as we have labels in the target variables.\n",
    "    #Importing LabelEncoder as xgboost understands numeric values and it helps in encoding the labels to numeric values.\n",
    "    #Importing accuracy_score for the accuracy of the model.\n",
    "    #We have used ravel() as the type of Y became a dataframe/column vector when we use iloc. \n",
    "    #The LabelEncoder was expecting either a Series/1d array, so we used ravel().\n",
    "    #Taking a copy of the dataframe.\n",
    "\n",
    "    df_c = df.copy()\n",
    "\n",
    "    import xgboost as xgb\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    #Creating OneHotEncoding as xgboost uses numerical values.\n",
    "    \n",
    "    onehotcols = [cols for cols in df_c.columns if df_c[cols].dtype==object]\n",
    "    df_c = pd.get_dummies(df_c, columns=onehotcols)\n",
    "\n",
    "    #Splitting the data into X and Y\n",
    "\n",
    "    X = df_c\n",
    "    Y = df_all.iloc[:,-1:]\n",
    "\n",
    "    #Creating an object of LabelEncoder and using the same to transform the target variable Y.\n",
    "    \n",
    "    Encoder = LabelEncoder()\n",
    "    Encoder = Encoder.fit(Y.values.ravel())\n",
    "    Encoder_Y = Encoder.transform(Y.values.ravel())\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Encoder_Y, test_size=.20, random_state=1)\n",
    "\n",
    "    #Creating and Training a model of XGBClassifier.\n",
    "    \n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train,Y_train)\n",
    "\n",
    "    #Predicting from the model and returning the accuracy score.\n",
    "    \n",
    "    pred=model.predict(X_test)\n",
    "    return accuracy_score(Y_test,pred)\n",
    "\n",
    "  def log_reg(self,df):\n",
    "    #Importing LogisticRegression.\n",
    "    #Importing accuracy_score for the accuracy of the model.\n",
    "    #We have used ravel() as the type of Y became a dataframe/column vector when we use iloc.\n",
    "    #Taking a copy of the dataframe. \n",
    "\n",
    "    df_c = df.copy()\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    #Creating OneHotEncoding.\n",
    "    \n",
    "    onehotcols = [cols for cols in df_c.columns if df_c[cols].dtype==object]\n",
    "    df_c = pd.get_dummies(df_c, columns=onehotcols)\n",
    "\n",
    "    #Splitting the data into X and Y\n",
    "    \n",
    "    X = df_c\n",
    "    Y = df_all.iloc[:,-1:].values.ravel()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.20,random_state=1)\n",
    "\n",
    "    #Creating and training a model of Logistic Regression.\n",
    "    \n",
    "    Model = LogisticRegression()\n",
    "    Model.fit(X_train,Y_train)\n",
    "\n",
    "    #Predicting the accuracy of the model.\n",
    "    \n",
    "    pred = Model.predict(X_test)\n",
    "    return accuracy_score(Y_test,pred)\n",
    "  \n",
    "  def knn_classifier(self,df):\n",
    "  \n",
    "    #Importing KNeighborsClassifier.\n",
    "    #Importing zscore for Z-Score normalization.\n",
    "    #Importing accuracy_score for the accuracy of the model.\n",
    "    #We have used ravel() as the type of Y became a dataframe/column vector when we use iloc.\n",
    "    #Taking a copy of the dataframe.\n",
    "\n",
    "    df_c = df.copy() \n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from scipy.stats import zscore\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    #Creating OneHotEncoding.\n",
    "    \n",
    "    onehotcols = [cols for cols in df_c.columns if df_c[cols].dtype==object]\n",
    "    df_c = pd.get_dummies(df_c, columns=onehotcols)\n",
    "\n",
    "    #Splitting the data into X and Y and applying zscore in X.\n",
    "    \n",
    "    X = df_c\n",
    "    Y = df_all.iloc[:,-1:].values.ravel()\n",
    "    X_Scaled = X.apply(zscore)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_Scaled,Y,test_size=0.20,random_state=1)\n",
    "\n",
    "    #Creating and training a model of KNN.\n",
    "    \n",
    "    Model = KNeighborsClassifier(n_neighbors = 5)\n",
    "    Model.fit(X_train,Y_train)\n",
    "\n",
    "    #Predicting the accuracy of the model.\n",
    "    \n",
    "    pred = Model.predict(X_test)\n",
    "    return accuracy_score(Y_test,pred)\n",
    "\n",
    "  def svm_classifier(self,df):\n",
    "    #Importing svm.\n",
    "    #Importing accuracy_score for the accuracy of the model.\n",
    "    #We have used ravel() as the type of Y became a dataframe/column vector when we use iloc.\n",
    "    #Taking a copy of the dataframe.\n",
    "\n",
    "    df_c = df.copy()\n",
    "\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    #Creating OneHotEncoding.\n",
    "    \n",
    "    onehotcols = [cols for cols in df_c.columns if df_c[cols].dtype==object]\n",
    "    df_c = pd.get_dummies(df_c, columns=onehotcols)\n",
    "\n",
    "    #Splitting the data into X and Y.\n",
    "    \n",
    "    X = df_c\n",
    "    Y = df_all.iloc[:,-1:].values.ravel()\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.20,random_state=1)\n",
    "\n",
    "    #Creating and training a model of KNN.\n",
    "    \n",
    "    Model = svm.SVC(gamma=0.025,C=3)\n",
    "    Model.fit(X_train,Y_train)\n",
    "\n",
    "    #Predicting the accuracy of the model.\n",
    "    \n",
    "    pred = Model.predict(X_test)\n",
    "    return accuracy_score(Y_test,pred)\n",
    "  \n",
    "  #We have defined a function called best_model which will call the different models we have defined.\n",
    "  #We will append their accuracy_score in a list and compare them with max(list).\n",
    "  #We will return the model with max accuracy_score to be the best model.\n",
    "\n",
    "  def best_model(self):\n",
    "    log = self.log_reg(df)\n",
    "    xg = self.xg_boost(df)\n",
    "    knn = self.knn_classifier(df)\n",
    "    svm = self.svm_classifier(df)\n",
    "    l = []\n",
    "    l.append(log)\n",
    "    l.append(xg)\n",
    "    l.append(knn)\n",
    "    l.append(svm)\n",
    "    if max(l)==log:\n",
    "      print(\"The best model for the given data is Logistic Regression with \",round(log*100,2),\"% accuracy\",sep='')\n",
    "    if max(l)==xg:\n",
    "      print(\"The best model for the given data is XGBoost with \",round(xg*100,2),\"% accuracy\",sep='')\n",
    "    if max(l)==knn:\n",
    "      print(\"The best model for the given data is KNN with \",round(knn*100,2),\"% accuracy\",sep='')\n",
    "    if max(l)==svm:\n",
    "      print(\"The best model for the given data is SVM with \",round(svm*100,2),\"% accuracy\",sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b0b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model for the given data is Logistic Regression with 81.19% accuracy\n"
     ]
    }
   ],
   "source": [
    "#Creating an object of the class Workflow and passing the dataframes.\n",
    "#Calling initialize function to initialize and main function.\n",
    "#Calling the best_model function to get the best model.\n",
    "\n",
    "ob = Workflow()\n",
    "ob.initialize(df3,df_original)\n",
    "ob.main()\n",
    "ob.best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef28f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
